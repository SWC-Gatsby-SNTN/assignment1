{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the psychometric function\n",
    "\n",
    "This jupyter notebook is intended to give background on the psychometric function, which is used in psychophysics to model responses in a two-alternative forced choice task. You can run all cells below by selecting the cell, and pressing shift-enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "plt.style.use('ggplot')\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a Gaussian distribution:\n",
    "\n",
    "$ f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{(x - \\mu)^2}{2 \\sigma^2} \\right)  $\n",
    "\n",
    "where $\\mu$ is the mean, and $\\sigma$ is the standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian(xs, mu, sigma):\n",
    "    return np.exp(-(xs - mu) ** 2 / (2 * sigma ** 2)) / np.sqrt(2 * np.pi * sigma ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_gaussian(mu, sigma):\n",
    "    plt.figure()\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(0, .4)\n",
    "    xs = np.linspace(-10, 10)\n",
    "    plt.plot(xs,gaussian(xs, mu, sigma))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell below will plot the Gaussian distribution interactively. By sliding the bars you can vary $\\mu$ and $\\sigma$ to see what happens the the shape of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_gaussian, mu=(-10., 10.), sigma=(1.,5.));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 2AFC task of estimating velocities, subjects sample their estimates of two Gaussian distributions $p(\\hat{v_1}|v_1)$ and $p(\\hat{v_2}|v_2)$, and compare their estimated $\\hat{v_1}$ with their estimated $\\hat{v_2}$. They are asked to specify which stimulus is fastest.\n",
    "\n",
    "This question can be posed mathematically as a forced choice answer to the question: Is $\\hat{v_2} > \\hat{v_1}$ ? Or, equally, is the difference $\\hat{v_2} - \\hat{v_1} > 0$ ?. What can we say about the distribution of possible values for $\\hat{v_2} - \\hat{v_1}$ ?\n",
    "\n",
    "A well-known (but somewhat tricky to derive) result is that the distribution of the difference between two Gaussian random variables is also a Gaussian, with the difference between the two means as mean, and the sum of the variances as variance. To find the probability that $p(\\hat{v_2}>\\hat{v_1})$ (or, the probability that the subject answers \"$\\hat{v_2}$ is faster\"), you just need to sum the (shaded) area under the curve of this difference distribution, for values of $\\hat{v_2} - \\hat{v_1}$ bigger than 0. \n",
    "\n",
    "In the cell below both an example $p(\\hat{v_1}|v_1)$ and $p(\\hat{v_2}|v_2)$ as well as the difference distribution are plotted. Note that although the true speeds are significantly different, the PDFs over observations are wide enough that when we sample from both PDFs there's a reasonable chance that the observed speeds could be \"the wrong way round\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdfs(true_v1=5, true_v2=8, \n",
    "              sigma_v1=1.4, sigma_v2=2):\n",
    "    plt.xlim(-5, 20)\n",
    "    plt.ylim(0, .5)\n",
    "    xs = np.linspace(-5, 20)\n",
    "    plt.plot(xs,gaussian(xs, mu=true_v1, sigma=sigma_v1))\n",
    "    plt.plot(xs,gaussian(xs, mu=true_v2, sigma=sigma_v2))\n",
    "    plt.xlabel('V')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend([\"Probability of observing $\\hat{v}_1$ given true $v_1 = $\" + str(true_v1),\n",
    "                \"Probability of observing $\\hat{v}_2$ given true $v_2 = $\" + str(true_v2)],\n",
    "                fontsize=14)\n",
    "\n",
    "def compute_prob_v2(xs, difference_dist):\n",
    "    binsize = np.diff(xs)[0]\n",
    "    idx_bigger_than_0 = np.where(xs>0)\n",
    "    return sum(difference_dist[idx_bigger_than_0] * binsize)    \n",
    "    \n",
    "def plot_pdfs_and_different(true_v1=5, true_v2=8, \n",
    "                            sigma_v1=1.4, sigma_v2=2):\n",
    "    plt.figure(figsize=(18, 6))    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_pdfs(true_v1, true_v2, sigma_v1, sigma_v2)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    standard_dev_of_difference = np.sqrt(sigma_v1 ** 2 + sigma_v2 ** 2)\n",
    "\n",
    "    xs = np.linspace(-5, 20)\n",
    "    difference = gaussian(xs, mu=true_v2 - true_v1, sigma=standard_dev_of_difference)\n",
    "    plt.plot(xs, difference)\n",
    "\n",
    "    probability_v2 = compute_prob_v2(xs, difference)\n",
    "    plt.fill_between(xs[np.where(xs>0)], difference[np.where(xs>0)], 0, color='m',alpha=.2)\n",
    "    plt.xlabel('$\\hat{v}_2 - \\hat{v}_1$')\n",
    "    plt.ylim([0, 0.2])\n",
    "    plt.legend(['Probability of observing $\\hat{v}_2 - \\hat{v}_1$',\n",
    "                'Overall chance of picking $V_2$: %.3f' % probability_v2], fontsize=14)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    diff_v2v1 = true_v2 - true_v1\n",
    "    return diff_v2v1, probability_v2\n",
    "    \n",
    "    \n",
    "    \n",
    "plot_pdfs_and_different();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the curve can be computed by solving the integral of the Gaussian distribution. This gives the chance that an observer will correctly pick $\\hat{v}_2$ as faster. This is computed numerically in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to compute this probability (that the observer answers $v_2$) for various true values of $v_1$ and $v_2$. The further apart the true values, or the lower the noise in the estimates, the less likely it is that we'll give the wrong answer. \n",
    "\n",
    "Let's see how the integral varies with different true values. Think about:\n",
    "- what happens if the two true values are identical? Does this behaviour depend on variances?\n",
    "- what happens if you keep the true values fixed (but not identical) and vary the variances?\n",
    "- what happens if you keep the variances fixed but vary the true values?\n",
    "- what happens when the variances are very small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_pdfs_and_different, \n",
    "         true_v1=(0,10,0.55), \n",
    "         true_v2=(0,10, 0.25), \n",
    "         sigma_v1=(.2,4), \n",
    "         sigma_v2=(.2,4));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The psychometric function\n",
    "A psychometric function tells us how often subjects choose $\\hat{v}_2$ depending on the difference between the two true values $v_1$ and $v_2$. You should have observed above that for fixed variances, varying the difference between the true values only affects the \"difference gaussian\" by shifting it along the x axis. This means that for every difference $(v_2 - v_1)$, we need to integrate the same gaussian between different limits. This gives us a \"cumulative distribution function\" for a Gaussian, which has a known form. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_integral(xs, mu, sigma):\n",
    "    # This function computes the cumulative numerical integral of a Gaussian distribution with \n",
    "    # mean mu and standard deviation sigma\n",
    "    binsize = np.diff(xs)[0]\n",
    "    integral = [sum(gaussian(xs[0:i], mu, sigma) * binsize) for i, x in enumerate(xs)]\n",
    "    # returns CDF\n",
    "    return np.array(integral)\n",
    "    \n",
    "def plot_psychometric(sigma_diff, title='The psychometric function'):\n",
    "    plt.title(title)\n",
    "    # CDF of zero-centred difference gaussian\n",
    "    xs = np.linspace(-10, 10)\n",
    "    plt.plot(xs, gaussian_integral(xs, 0, sigma_diff))\n",
    "    plt.xlabel('True $v_2 - v_1$')\n",
    "    plt.ylabel('Chance of picking $v_2$ as faster')\n",
    "\n",
    "def plot_pdfs_diff_and_psychometric(true_v1=5, true_v2=8):\n",
    "    sigma_v1 = 2\n",
    "    sigma_v2 = 3\n",
    "    diff_v2v1, probability_v2 = plot_pdfs_and_different(true_v1, true_v2, sigma_v1, sigma_v2)\n",
    "        \n",
    "    plt.figure()\n",
    "    sigma_diff = np.sqrt(sigma_v1 ** 2 + sigma_v2 ** 2)        \n",
    "    plot_psychometric(sigma_diff)\n",
    "    plt.scatter(diff_v2v1, probability_v2, s=200)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interact(plot_pdfs_diff_and_psychometric, \n",
    "         true_v1=(0,10,0.5), \n",
    "         true_v2=(0,10, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how our psychometric curve depends on the variance of the two estimators...\n",
    "\n",
    "- what happens if the variance of one distribution goes up or down?\n",
    "- are there differences in the effects of $\\sigma_1$ and $\\sigma_2$?\n",
    "- reflect on what the variance parameters might mean in terms of the experiment described in question 1.2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdfs_psychometric(sigma_v1, sigma_v2):\n",
    "    \n",
    "    true_v1 = 5\n",
    "    true_v2 = 8\n",
    "    plt.figure(figsize=(18, 6))    \n",
    "    plt.subplot(1,2,1)\n",
    "    plot_pdfs(true_v1, true_v2, \n",
    "              sigma_v1, sigma_v2)\n",
    "    plt.title(\"Example observation PDFs for one set of true $(v_1, v_2)$\")\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    sigma_diff = np.sqrt(sigma_v1 ** 2 + sigma_v2 ** 2)            \n",
    "    plot_psychometric(sigma_diff,title='The psychometric function for a fixed difference in true velocities of 3')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "interact(plot_pdfs_psychometric, \n",
    "         sigma_v1=(.2,4), \n",
    "         sigma_v2=(.2,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
